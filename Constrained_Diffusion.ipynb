{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "#If you don't comment next line, CUDA will only use GPU1 and GPU0 will be invisible\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(0)\n",
    "from dataclasses import dataclass\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "from diffusers import UNet2DModel\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers import DDPMScheduler\n",
    "import torch.nn.functional as F\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers import DDIMPipeline\n",
    "from diffusers.utils import make_image_grid\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from accelerate import notebook_launcher\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision\n",
    "from github import Github\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, ViTImageProcessor\n",
    "import shutil\n",
    "from itertools import cycle\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration of Training hyperparameters\n",
    "@dataclass\n",
    "\n",
    "class TrainingConfig:\n",
    "\n",
    "    adaptation = False #If adaptation = True, the problem will be treated as a adaptation/fine-tuning problem where we want to adapt a pre-trained model to new data without overfitting.\n",
    "\n",
    "    image_size = 32 # the generated image resolution (the training data will be resized to image_size*image_size)\n",
    "    latent_size = 32 # Latent resolution (ignore if not using latent diffusion)\n",
    "    diffusion_channels = 1 #1 for b&w images, 3 for RGB, 4 or more for latent diffusion\n",
    "\n",
    "    primal_batch_size = 128 #number of images in each mini-batch sampled for computing the primal loss\n",
    "    dual_batch_size = 64 #number of images in each mini-batch sampled for computing each constraint loss\n",
    "    eval_batch_size = 64 # how many samples to sample during evaluation step\n",
    "\n",
    "    num_epochs = 30 #number of total epochs\n",
    "    batches_per_epoch = 4 #number of mini-batches per epoch\n",
    "    primal_per_dual = 5 #number of primal descent steps after each update of the dual variables\n",
    "    \n",
    "    save_model_epochs = 30 # number of epochs between each time the model is saved\n",
    "    save_plot_epochs = 20 # number of epochs between each time plots of relevant variables are saved\n",
    "    save_image_epochs = 10 # number of epochs between each time a batch of images generated by the diffusion model are saved\n",
    "    running_average_length = 5 #length of the running average for plotting average histograms of generated samples during training\n",
    "\n",
    "    num_gpus = 2 #number of gpus to split the training on using the 'accelerate' library\n",
    "\n",
    "    load_model_header = 'MODEL_NAME' # header of the initial model to load from the 'save_models_dir' directory. used if continuing training of a previusly trained model or fine-tuning a pre-trained model.\n",
    "    save_model_header = 'saved' # header to save the model with \n",
    "\n",
    "    gradient_accumulation_steps = 1\n",
    "\n",
    "    lr_primal = 1e-4 # the maximum primal learning rate\n",
    "    lr_dual_to_primal = 1000 #ratio of dual learning rate to primal learning rate\n",
    "    lr_warmup_steps = 500 # number of warmup steps to use in the learning rate scheduler\n",
    "\n",
    "    evaluate = True #set to True if you want the model to sample images from the diffusion model after every #save_image_epochs steps.\n",
    "    wandb_logging = False #set to True if you want to log relevant variables to wandb\n",
    "\n",
    "    architecture_size = 128 # the size of the denoising U-net model can be scaled up or down using this parameter\n",
    "\n",
    "    \n",
    "    dataset_name = 'mnist' #name of the dataset to use for training. could be one of ['mnist', 'celeb-a', 'image-net']\n",
    "\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "\n",
    "    upload_to_github = True #If True, in addition to saving plots and imgs to file, will upload them to github repo\n",
    "    github_token = \" \"\n",
    "    github_repo = \" \"\n",
    "\n",
    "    output_dir = \"FOLDER_NAME\"  # the local directory name to save everything\n",
    "    save_models_dir = \"models\" # the local directory to save trained models\n",
    "\n",
    "config = TrainingConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier and VAE Setup\n",
    "\n",
    "#here we load a classifier corresponding to the dataset that is being used. For image-net we use a CLIP model as classifier and also load a pre-trained Variational AutoEncoder.\n",
    "\n",
    "if config.dataset_name == 'mnist':\n",
    "\n",
    "    path = 'farleyknight-org-username/vit-base-mnist'\n",
    "    classifier = ViTForImageClassification.from_pretrained(path)\n",
    "    classifier_processor = ViTImageProcessor.from_pretrained(path)\n",
    "    classifier.eval()\n",
    "\n",
    "elif config.dataset_name == 'celeb-a':\n",
    "\n",
    "    path = 'cledoux42/GenderNew_v002'\n",
    "    classifier = ViTForImageClassification.from_pretrained(path)\n",
    "    classifier_processor = ViTImageProcessor.from_pretrained(path)\n",
    "    classifier.eval()\n",
    "\n",
    "elif config.dataset_name == 'image-net':\n",
    "\n",
    "    #load CLIP model as classifier\n",
    "    clipmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    classifier_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    classifier = clipmodel\n",
    "    classifier.eval()\n",
    "\n",
    "    #load VAE\n",
    "    \n",
    "    vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\")\n",
    "    model_encoder = vae.encode\n",
    "    model_decoder = vae.decode\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Github Function\n",
    "def up_to_hub(image_path, content_path = 'images/uploaded_image.png', token = config.github_token, repo_name = config.github_repo):\n",
    "    #content_path is Path where you want the image to be uploaded in your repo\n",
    "\n",
    "    if config.upload_to_github == False:\n",
    "        return 1\n",
    "    \n",
    "    # Initialize Github instance with your token\n",
    "    g = Github(token)\n",
    "\n",
    "    # Get the specific repo\n",
    "    repo = g.get_repo(repo_name)\n",
    "\n",
    "    # Read the image as binary\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        data = image_file.read()\n",
    "        # Encode the binary data to base64 string\n",
    "        data_base64 = data\n",
    "        # data_base64 = base64.b64encode(data).decode()\n",
    "        \n",
    "    # Commit message\n",
    "    commit_message = 'Upload image via PyGithub'\n",
    "\n",
    "    # Upload the image\n",
    "    try:\n",
    "        # Check if the file already exists\n",
    "        contents = repo.get_contents(content_path)\n",
    "        repo.update_file(contents.path, commit_message, data_base64, contents.sha, branch=\"main\")\n",
    "        print(f\"Updated existing file: {content_path}\")\n",
    "    except:\n",
    "        # If the file does not exist, create it\n",
    "        repo.create_file(content_path, commit_message, data_base64, branch=\"main\")\n",
    "        print(f\"Created new file: {content_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Function \n",
    "def evaluate(running_count, config, epoch, pipeline, mu, c, git_folder, accelerator, device, classifier_processor, classifier, classify = False, upload_images = True, final_epoch = False):\n",
    "\n",
    "    if config.dataset_name != 'image-net':\n",
    "        \n",
    "        #sample images, in default PIL format\n",
    "        imgs = pipeline(\n",
    "        batch_size = config.eval_batch_size\n",
    "        ).images\n",
    "\n",
    "        if config.dataset_name == 'mnist':\n",
    "    \n",
    "            imgs_ = [np.array(img) for img in imgs]\n",
    "            imgs_ = np.stack(imgs_, axis = 0)\n",
    "            imgs_ = np.expand_dims(imgs_, axis = 1)\n",
    "            imgs_ = np.concatenate((imgs_, imgs_, imgs_), axis = 1)\n",
    "            imgs_ = torch.from_numpy(imgs_)\n",
    "            inputs = classifier_processor.preprocess(imgs_, do_rescale = True)\n",
    "\n",
    "\n",
    "        elif config.dataset_name == 'celeb-a':\n",
    "\n",
    "            imgs = pipeline(\n",
    "                batch_size = config.eval_batch_size, output_type = 'nd.array', num_inference_steps = 50\n",
    "                ).images\n",
    "    \n",
    "            imgs = torch.from_numpy(imgs)\n",
    "            imgs = torch.permute(imgs, (0, 3, 1, 2))\n",
    "            imgs = imgs.to(device)\n",
    "\n",
    "            inputs = classifier_processor.preprocess(imgs, do_rescale = False)\n",
    "            \n",
    "        \n",
    "        outputs = classifier(torch.tensor(inputs['pixel_values']).to(device))\n",
    "        logits_per_image = outputs.logits\n",
    "        probs = logits_per_image.softmax(dim = 1)\n",
    "\n",
    "        predicted_nums = torch.argmax(probs, dim = 1)\n",
    "        arr = predicted_nums.cpu().numpy()\n",
    "\n",
    "        nclasses = np.shape(running_count)[0]\n",
    "\n",
    "        counts = np.bincount(arr, minlength = nclasses)\n",
    "\n",
    "        running_count += counts\n",
    "        \n",
    "        if upload_images == True:\n",
    "\n",
    "            plt.bar(np.arange(0, nclasses, 1), counts, align='center')\n",
    "            plt.gca().set_xticks(np.arange(0, nclasses, 1))\n",
    "\n",
    "            plt.title('histogram of generated sample classes')\n",
    "            plt.savefig('class histogram.png')\n",
    "            plt.close()\n",
    "\n",
    "            up_to_hub('class histogram.png', content_path = git_folder_path + f\"/{epoch:04d}_histogram.png\")\n",
    "\n",
    "        if (int((epoch + 1) / config.save_image_epochs)%config.running_average_length == 0) and (upload_images == True):\n",
    "            plt.bar(np.arange(0, nclasses, 1), running_count, align='center', color = 'tab:red')\n",
    "            plt.gca().set_xticks(np.arange(0, nclasses, 1))\n",
    "\n",
    "            plt.title('running sum histogram of generated sample classes')\n",
    "            plt.savefig('running sum class histogram.png')\n",
    "            plt.close()\n",
    "\n",
    "            up_to_hub('running sum class histogram.png', content_path = git_folder_path + f\"/{epoch:04d}_rs_histogram.png\")\n",
    "            # running_count = 0\n",
    "        \n",
    "        if final_epoch == True:\n",
    "            plt.bar(np.arange(0, nclasses, 1), running_count, align='center', color = 'olivedrab')\n",
    "            plt.gca().set_xticks(np.arange(0, nclasses, 1))\n",
    "\n",
    "            plt.title('final_epoch histogram of generated sample classes')\n",
    "            plt.savefig('running sum class histogram.png')\n",
    "            plt.close()\n",
    "\n",
    "            up_to_hub('running sum class histogram.png', content_path = git_folder_path + '/final_epoch_histogram.png')\n",
    "\n",
    "        rows = int(np.sqrt(config.eval_batch_size))\n",
    "        cols = int(np.sqrt(config.eval_batch_size))\n",
    "\n",
    "        # imgs = imgs.permute(0, 3, 1, 2)\n",
    "        if config.dataset_name == 'celeb-a':\n",
    "            imgs_decoded = [torchvision.transforms.functional.to_pil_image(imgs[k, :, :, :]) for k in range((imgs.shape)[0])]\n",
    "\n",
    "            # Make a grid out of the images\n",
    "            image_grid = make_image_grid(imgs_decoded, rows, cols)\n",
    "        \n",
    "        else:\n",
    "            image_grid = make_image_grid(imgs, rows, cols)\n",
    "\n",
    "        # Save the images\n",
    "        test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "        os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "        image_path = f\"{test_dir}/{epoch:04d}.png\"\n",
    "\n",
    "        image_grid.save(image_path)\n",
    "\n",
    "        up_to_hub(image_path, content_path = git_folder_path + f\"/{epoch:04d}.png\")\n",
    "\n",
    "    ###################################################\n",
    "    #######################LATENT######################\n",
    "    ###################################################\n",
    "\n",
    "    elif config.dataset_name == 'image-net':\n",
    "\n",
    "        device = accelerator.device\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            imgs = pipeline(\n",
    "            batch_size = config.eval_batch_size, output_type = 'nd.array', num_inference_steps = 50\n",
    "            ).images\n",
    "    \n",
    "            imgs = torch.from_numpy(imgs)\n",
    "            imgs = torch.permute(imgs, (0, 3, 1, 2))\n",
    "            imgs = imgs.to(device)\n",
    "\n",
    "            # print('a1')\n",
    "\n",
    "            # imgs = (imgs - 0.675)*50\n",
    "            \n",
    "            # print('a2')\n",
    "            \n",
    "\n",
    "            vae.to(device)\n",
    "            # print('a4')\n",
    "\n",
    "            model_decoder = vae.decode\n",
    "\n",
    "            # print('a6')\n",
    "\n",
    "            print(imgs.shape)\n",
    "            \n",
    "            a = 0.431\n",
    "            b = 36\n",
    "            imgs_ = (model_decoder((imgs - a)*b).sample).detach()\n",
    "\n",
    "            # imgs_ = imgs_ - torch.mean(imgs_) + torch.mean(batch.to(device))\n",
    "\n",
    "            print(imgs_.shape)\n",
    "\n",
    "            #Classify\n",
    "            # print('classify start')\n",
    "\n",
    "            imgs_ = torch.clip((imgs_.permute(0, 2, 3, 1)), 0, 1)\n",
    "\n",
    "            # print('c1')\n",
    "  \n",
    "            labels = ['photo of a cassette player', 'photo of a tench fish', 'photo of a garbage truck', 'photo of a parachute', 'photo of a fench horn', 'photo of a english springer dog', 'photo of a golf ball', 'photo of a church', 'photo of a gas pump', 'photo of a chainsaw']\n",
    "            shorts = ['cass', 'fish', 'truck', 'parach', 'horn', 'dog', 'ball', 'church', 'pump', 'saw']\n",
    "\n",
    "            classifier_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", device_map = device)\n",
    "            clipmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", device_map = device)\n",
    "            # print('c1.5')\n",
    "        \n",
    "            imgs_ = imgs_.to(device)\n",
    "            \n",
    "            # print('c1.6')\n",
    "\n",
    "            print(imgs_.device)\n",
    "\n",
    "            inputs = classifier_processor(text = labels, images = imgs_, do_rescale = False, do_convert_rgb = True, return_tensors = \"np\", padding = True)\n",
    "\n",
    "            # print('c2')\n",
    "\n",
    "\n",
    "            inputs['pixel_values'] = torch.from_numpy(inputs['pixel_values']).float().to(device)\n",
    "            inputs['attention_mask'] = torch.from_numpy(inputs['attention_mask']).int().to(device)\n",
    "            inputs['input_ids'] = torch.from_numpy(inputs['input_ids']).int().to(device)\n",
    "\n",
    "            # print('c2.5')\n",
    "            outputs = clipmodel(**inputs)\n",
    "            # print('c3')\n",
    "            logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "            # print('c3.5')\n",
    "            probs = logits_per_image.softmax(dim = 1)  # we can take the softmax to get the label probabilities\n",
    "            # print('c4')\n",
    "            predicted_nums = torch.argmax(probs, dim = 1)\n",
    "            arr = predicted_nums.cpu().numpy()\n",
    "            counts = np.bincount(arr, minlength = 10)\n",
    "\n",
    "            running_count += counts\n",
    "\n",
    "            plt.bar(np.arange(0, 10), counts, align='center')\n",
    "            plt.gca().set_xticks(np.arange(0, 10), shorts)\n",
    "\n",
    "            plt.title('histogram of generated sample classes')\n",
    "            plt.savefig('class histogram.png')\n",
    "            plt.close()\n",
    "\n",
    "            up_to_hub('class histogram.png', content_path = git_folder_path + f\"/{epoch:04d}_histogram.png\")\n",
    "\n",
    "            rows = int(np.sqrt(config.eval_batch_size))\n",
    "            cols = int(np.sqrt(config.eval_batch_size))\n",
    "\n",
    "        ###########################################################\n",
    "        #Save and Upload the generated Latents\n",
    "        imgs_latents = [torchvision.transforms.functional.to_pil_image(imgs[k, :, :, :]) for k in range((imgs.shape)[0])]\n",
    "        image_grid = make_image_grid(imgs_latents, rows, cols)\n",
    "\n",
    "        test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "        os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "        image_path = f\"{test_dir}/{epoch:04d}.png\"\n",
    "\n",
    "        image_grid.save(image_path)\n",
    "        \n",
    "        up_to_hub(image_path, content_path = git_folder + f\"/{epoch:04d}_latents.png\")\n",
    "        ###########################################################\n",
    "        #Save and Upload the generated images (decoded latents)\n",
    "        imgs_ = imgs_.permute(0, 3, 1, 2)\n",
    "        imgs_decoded = [torchvision.transforms.functional.to_pil_image(imgs_[k, :, :, :]) for k in range((imgs_.shape)[0])]\n",
    "        image_grid = make_image_grid(imgs_decoded, rows, cols)\n",
    "        # Save the images\n",
    "        test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "        os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "        image_path = f\"{test_dir}/{epoch:04d}.png\"\n",
    "        image_grid.save(image_path)\n",
    "        \n",
    "        up_to_hub(image_path, content_path = git_folder + f\"/{epoch:04d}.png\")\n",
    "        ###########################################################\n",
    "        #Save and Upload the latent pixel values histogram\n",
    "\n",
    "\n",
    "        bat = (torch.flatten(imgs[0, :, :, :])).cpu().detach()\n",
    "        lat = bat.numpy()\n",
    "        plt.figure()\n",
    "        plt.hist(lat, bins = 50)\n",
    "\n",
    "        plt.savefig('lat.png')\n",
    "        image_path = 'lat.png'\n",
    "        up_to_hub(image_path, content_path = git_folder + f\"/{epoch:04d}_latents_histogram.png\")\n",
    "        plt.close()\n",
    "        ###########################################################\n",
    "\n",
    "    return running_count\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform = None, target_transform = None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        N = int(torch.load(self.img_dir + '/N').item())\n",
    "\n",
    "        return N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, '/sample_' + str(idx) + '.pt')\n",
    "        img_tensor = torch.load(self.img_dir + '/sample_' + str(idx) + '.pt')\n",
    "\n",
    "        image = img_tensor\n",
    "\n",
    "        label = 1\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fast Dataset Creation Function\n",
    "def create_datasets_fast(N_spc, config, dataloaders_only = False, save_jpgs_only = False):\n",
    "    \n",
    "    #################################\n",
    "    #Delete and recreate dataset directories\n",
    "    N_datasets = np.shape(N_spc)[0]\n",
    "    N_classes = np.shape(N_spc)[1]\n",
    "\n",
    "    dataset_names_list = []\n",
    "    \n",
    "    if os.path.exists('temp_data'):\n",
    "        shutil.rmtree('temp_data')\n",
    "    os.mkdir('temp_data')\n",
    "\n",
    "    for i in range(N_datasets):\n",
    "        dataset_names_list += [\"dataset_\" + str(i)]\n",
    "    \n",
    "    if save_jpgs_only == True:\n",
    "        dataset_names_list = [\"FID_baseline_dataset_\" + config.dataset_name]\n",
    "\n",
    "    if dataloaders_only == False:\n",
    "\n",
    "        for new_folder_name in dataset_names_list:\n",
    "            # Specify the name of the new folder\n",
    "            if os.path.exists(new_folder_name):\n",
    "                shutil.rmtree(new_folder_name)\n",
    "            # Create a new directory in the current working directory\n",
    "            os.mkdir(new_folder_name)\n",
    "            # print(f\"Folder '{new_folder_name}' created successfully.\")\n",
    "\n",
    "        #################################\n",
    "        #create datasets\n",
    "        if config.dataset_name == 'image-net':\n",
    "            dataset_orig = load_dataset(\"frgfm/imagenette\", '320px', split=\"train\")\n",
    "        elif config.dataset_name == 'mnist':\n",
    "            dataset_orig = load_dataset(\"ylecun/mnist\", split=\"train\")\n",
    "        elif config.dataset_name == 'celeb-a':\n",
    "            dataset_orig = load_dataset(\"tpremoli/CelebA-attrs\", split=\"validation\")\n",
    "            dataset_orig = dataset_orig.rename_column('Male', 'label')\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "        for i in range(N_datasets):\n",
    "\n",
    "            path = dataset_names_list[i]\n",
    "            n = 0\n",
    "\n",
    "            for j in range(N_classes):\n",
    "\n",
    "                if N_spc[i, j] == 0:\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                if config.dataset_name == 'celeb-a':\n",
    "                    ds_filtered = dataset_orig.filter(lambda example: example[\"label\"] == int((j-0.5)*2))\n",
    "                else:\n",
    "                    ds_filtered = dataset_orig.filter(lambda example: example[\"label\"] == j)\n",
    "                ds_filtered = ds_filtered.filter(lambda example, idx: idx < N_spc[i, j], with_indices=True)\n",
    "                ds_filtered = ds_filtered.with_format(\"torch\")\n",
    "            \n",
    "\n",
    "                dataloader = DataLoader(ds_filtered, batch_size=1)\n",
    "\n",
    "                trans = torchvision.transforms.Resize((config.image_size, config.image_size))\n",
    "\n",
    "                \n",
    "                for batch in dataloader:\n",
    "                    \n",
    "                    img = trans(batch['image'][0, :, :, :])\n",
    "\n",
    "                    if img.shape[0] == 1 and config.dataset_name != 'mnist':\n",
    "                        img = img.repeat(3, 1, 1)\n",
    "        \n",
    "                    if config.dataset_name == 'image-net':\n",
    "                        torch.save(img, 'temp_data/' + 'sample_' + str(n) + '.pt')\n",
    "                    if config.dataset_name != 'image-net' and save_jpgs_only == False:\n",
    "                        img = img/255\n",
    "                        # print(img)\n",
    "                        torch.save(img, path + '/sample_' + str(n) + '.pt')\n",
    "                        img = img*255\n",
    "\n",
    "                    if save_jpgs_only == True:\n",
    "                        img = img/255\n",
    "                        img = img.double()\n",
    "                        # if j == 0:\n",
    "                        #     plt.imshow(img.permute(1, 2, 0).detach())\n",
    "                        #     plt.show()\n",
    "                        #     assert False\n",
    "                        torchvision.utils.save_image(img, path + '/sample_' + str(n) + '.jpg')\n",
    "                    \n",
    "                    n += 1\n",
    "\n",
    "                N = torch.tensor(n)\n",
    "                torch.save(N, 'temp_data/N')\n",
    "                torch.save(N, path + '/N')\n",
    "\n",
    "                if save_jpgs_only == True:\n",
    "                    continue\n",
    "\n",
    "                if config.dataset_name == 'image-net':    \n",
    "\n",
    "                    ds = CustomImageDataset(img_dir = 'temp_data')\n",
    "                    dataloader = torch.utils.data.DataLoader(ds, batch_size = 16, shuffle = True)\n",
    "\n",
    "                    device ='cuda:0'\n",
    "                    vae.to(device)\n",
    "                    model_encoder = vae.encode\n",
    "\n",
    "                    n = 0\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for batch in dataloader:\n",
    "                            batch[0] = (batch[0]/255).to(device).detach()\n",
    "                            lat = model_encoder(batch[0])\n",
    "                            lat = lat.to_tuple()[0]\n",
    "                            lat = (lat.mean).detach()\n",
    "                            for k in range(lat.shape[0]):\n",
    "                                torch.save(lat[k, :, :, :].detach(), path + '/sample_' + str(n) + '.pt')\n",
    "                                n += 1\n",
    "                \n",
    "                \n",
    "            N = torch.tensor(n)\n",
    "\n",
    "            torch.save(N, path + '/N')\n",
    "    #Now we create the dataloaders\n",
    "    dataset_list = []\n",
    "    preprocess = transforms.Compose(\n",
    "    [\n",
    "        # transforms.Resize((config.image_size, config.image_size)),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        # transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    "    )   \n",
    "\n",
    "    for i in range(N_datasets):\n",
    "        if config.dataset_name != 'image-net':\n",
    "            dataset_list += [CustomImageDataset(img_dir = dataset_names_list[i], transform = preprocess)]\n",
    "        else:\n",
    "            dataset_list += [CustomImageDataset(img_dir = dataset_names_list[i])]\n",
    "\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    train_dataloader_list = []\n",
    "\n",
    "    for i in range(len(dataset_list)):\n",
    "        train_dataloader_list += [torch.utils.data.DataLoader(dataset_list[i], batch_size = config.primal_batch_size, shuffle = True)]\n",
    "\n",
    "        \n",
    "    return train_dataloader_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Setup\n",
    "\n",
    "#A 2D UNet model that takes a noisy sample and a timestep and returns a sample shaped output.\n",
    "model = UNet2DModel(\n",
    "    sample_size = config.latent_size,  # the target image resolution\n",
    "    in_channels = config.diffusion_channels,  # the number of input channels, 3 for RGB images\n",
    "    out_channels = config.diffusion_channels,  # the number of output channels, 3 for RGB images\n",
    "    layers_per_block = 2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels = (config.architecture_size, config.architecture_size, 2*config.architecture_size, 2*config.architecture_size, 4*config.architecture_size, 4*config.architecture_size),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "if config.adaptation == True:\n",
    "\n",
    "    model_pretrained = UNet2DModel(\n",
    "        sample_size = config.latent_size,  # the target image resolution\n",
    "        in_channels = config.diffusion_channels,  # the number of input channels, 3 for RGB images\n",
    "        out_channels = config.diffusion_channels,  # the number of output channels, 3 for RGB images\n",
    "        layers_per_block = 2,  # how many ResNet layers to use per UNet block\n",
    "        block_out_channels = (config.architecture_size, config.architecture_size, 2*config.architecture_size, 2*config.architecture_size, 4*config.architecture_size, 4*config.architecture_size),  # the number of output channels for each UNet block\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "            \"DownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "            \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "#loading parameters of pre-trained model if needed\n",
    "if config.load_model_header != 'None':\n",
    "    device = torch.device('cpu')\n",
    "    state_dict = torch.load(config.save_models_dir + '/trained_model_' + config.load_model_header + '.pt', map_location = device)\n",
    "    # create new OrderedDict that does not contain `module.`\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    # load params\n",
    "\n",
    "    if config.adaptation == True:\n",
    "        model_pretrained.load_state_dict(new_state_dict)\n",
    "        model_pretrained.eval()\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    \n",
    "\n",
    "    print('loaded model ' + config.load_model_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop function\n",
    "def train_loop(config, git_folder_path, model, classifier, noise_scheduler, optimizer, train_dataloaders, lr_scheduler, mu_init, b_init, constrained = False, resilient = False, alpha = 0,  model_pretrained = None):\n",
    "    \n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "\n",
    "    device = accelerator.device\n",
    "\n",
    "    #initialize lagrange multipliers and constraint thresholds\n",
    "    print(mu_init)\n",
    "    print(b_init)\n",
    "    mu = mu_init.to(device)\n",
    "    b = b_init.to(device)\n",
    "\n",
    "    model, optimizer, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, lr_scheduler\n",
    "    )\n",
    "\n",
    "    #Nc is the number of constraints here\n",
    "    Nc = len(train_dataloaders)\n",
    "\n",
    "    if constrained == False:\n",
    "        Nc = 1\n",
    "\n",
    "    mu[0] = 1\n",
    "    \n",
    "    dataloaders = []\n",
    "    iterators = []\n",
    "\n",
    "    #prepare all the dataloaders\n",
    "    for k in range(Nc):\n",
    "        dataloaders += [accelerator.prepare(train_dataloaders[k])]\n",
    "        iterators += [cycle(dataloaders[k])]\n",
    "\n",
    "    \n",
    "    if config.adaptation == True:\n",
    "        Nc = 2\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "\n",
    "    #initialize history tensors\n",
    "    mu_hist = torch.zeros((Nc, int(config.num_epochs/config.primal_per_dual)))\n",
    "    b_hist = torch.zeros((Nc, int(config.num_epochs/config.primal_per_dual)))\n",
    "    constraint_hist = torch.zeros((Nc, int(config.num_epochs/config.primal_per_dual)))\n",
    "    lagrangian_hist = torch.zeros(int(config.num_epochs/config.primal_per_dual))\n",
    "    running_count_hist = np.zeros((int(int(config.num_epochs / config.save_image_epochs)/config.running_average_length), 10))\n",
    "\n",
    "    \n",
    "    i_hist = 0\n",
    "    j = 0\n",
    "\n",
    "    mu_hist[:, i_hist] = mu\n",
    "    b_hist[:, i_hist] = b\n",
    "\n",
    "    running_count = np.zeros(10)\n",
    "\n",
    "\n",
    "    #normalizing constants used for image-net dataset since the autoencoder used is not properly normalized\n",
    "\n",
    "    if config.dataset_name == 'image-net':\n",
    "        a = -15\n",
    "        b = 20\n",
    "        gamma = 2/(b - a)\n",
    "        beta = -(a + b)/(b - a)\n",
    "        a = 0.431\n",
    "        b = 36\n",
    "    else:\n",
    "        gamma = 1\n",
    "        beta = 0\n",
    "\n",
    "    if config.adaptation == True:\n",
    "        with torch.no_grad():\n",
    "            model_pretrained = model_pretrained.to(device)\n",
    "    \n",
    "    \n",
    "\n",
    "    #Actually start training\n",
    "    for epoch in range(config.num_epochs):\n",
    "\n",
    "        progress_bar = tqdm(total = config.batches_per_epoch, disable = not accelerator.is_local_main_process, position = 0)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "\n",
    "        for step in range(config.batches_per_epoch):\n",
    "\n",
    "            Clean_images = []\n",
    "            Noises = []\n",
    "            Batch_sizes = []\n",
    "            Timesteps = []\n",
    "            Noisy_images = []\n",
    "            Noise_preds = []\n",
    "\n",
    "            for k in range(Nc):\n",
    "\n",
    "                device = accelerator.device\n",
    "\n",
    "                train_features = next(iterators[k])[0]\n",
    "                Clean_images += [((train_features*gamma) + beta).to(device)]\n",
    "                Noises += [torch.randn(Clean_images[k].shape, device = Clean_images[k].device)]\n",
    "                Batch_sizes += [Clean_images[k].shape[0]]\n",
    "                Timesteps += [torch.randint(\n",
    "                            0, noise_scheduler.config.num_train_timesteps, (Batch_sizes[k],), device = Clean_images[k].device,\n",
    "                            dtype = torch.int64\n",
    "                        )]\n",
    "                Noisy_images += [noise_scheduler.add_noise(Clean_images[k], Noises[k], Timesteps[k])]\n",
    "\n",
    "                if config.adaptation == True and constrained == True:\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "\n",
    "                Losses = []\n",
    "\n",
    "                #The Lagrangian\n",
    "                lag = torch.zeros(1, requires_grad = False).to(device)\n",
    "\n",
    "                for l in range(Nc):\n",
    "\n",
    "                    Noise_preds += model(Noisy_images[l], Timesteps[l], return_dict = False)\n",
    "\n",
    "                    Losses += [F.mse_loss(Noise_preds[l], Noises[l])]\n",
    "\n",
    "                    #The Lagrangian\n",
    "                    lag += mu[l]*Losses[l]\n",
    "\n",
    "                    if constrained == False:\n",
    "                        if mu[l] != 1:\n",
    "                            print('ERROR: Unconstrained Lagrange multiplier should be equal to 1')\n",
    "                        break\n",
    "\n",
    "                    if config.adaptation == True:\n",
    "                        \n",
    "                        Noise_preds += model_pretrained(Noisy_images[0], Timesteps[0], return_dict = False)\n",
    "\n",
    "                        Losses += [F.mse_loss(Noise_preds[1], Noises[0])]\n",
    "\n",
    "                        lag += mu[1]*Losses[1]\n",
    "\n",
    "                        break\n",
    "\n",
    "\n",
    "                accelerator.backward(lag)\n",
    "                \n",
    "                if ((epoch + 1)%config.primal_per_dual == 0):\n",
    "                    lagrangian_hist[i_hist] = lag.detach()\n",
    "                \n",
    "                ############### DUAL STEP START #############\n",
    "                current_lr = lr_scheduler.get_last_lr()[0]\n",
    "                lr_dual = config.lr_dual_to_primal*current_lr\n",
    "\n",
    "                if (epoch + 1)%config.primal_per_dual == 0 and constrained == True:\n",
    "\n",
    "                    l = torch.tensor(Losses, requires_grad = False).to(device)\n",
    "\n",
    "                    constraint_hist[:, i_hist] = (l - b).detach()\n",
    "\n",
    "                    if resilient == False:\n",
    "\n",
    "                        mu = (mu + lr_dual*(l - b)).detach()\n",
    "\n",
    "                    elif resilient == True:\n",
    "                        \n",
    "                        mu = (mu + lr_dual*(l - 2*alpha*mu)).detach()\n",
    "\n",
    "                    mu = (torch.nn.functional.relu(mu, inplace = True)).detach()\n",
    "\n",
    "                    mu[0] = 1\n",
    "\n",
    "                    mu = accelerator.reduce(mu, reduction = 'mean')\n",
    "\n",
    "                    mu_hist[:, i_hist] = mu.detach()\n",
    "                ################ DUAL STEP END ###############\n",
    "\n",
    "                \n",
    "                    \n",
    "                if accelerator.is_main_process and (config.wandb_logging == True):\n",
    "                \n",
    "                    wandb.log({'Lagrangian': lag.detach().item()})\n",
    "                    wandb.log({'lr_dual': lr_dual})\n",
    "                    wandb.log({'lr_primal': current_lr})\n",
    "\n",
    "                    for k in range(Nc):\n",
    "                        wandb.log({('mu_' + str(k)) : mu[k].detach().item()})\n",
    "                        wandb.log({('loss_' + str(k)) : Losses[k].detach().item()})\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": lag.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "\n",
    "        if accelerator.is_main_process and config.evaluate == True:\n",
    "            \n",
    "            classifier = classifier.to(device)\n",
    "\n",
    "            ##important\n",
    "            pipeline = DDIMPipeline(unet = accelerator.unwrap_model(model), scheduler = noise_scheduler)\n",
    "            \n",
    "            \n",
    "            if ((epoch + 1) % config.save_image_epochs == 0) and ((epoch + 1) % config.save_plot_epochs != 0):\n",
    "                #just draw samples and show them in a grid\n",
    "                running_count = evaluate(running_count, config, epoch, pipeline, mu_init, b_init, git_folder_path, accelerator, device, classifier_processor, classifier, classify = True)\n",
    "\n",
    "            if epoch == config.num_epochs - 1:\n",
    "                #In the last epoch sample lots of images and check their classes\n",
    "                running_count_final_epoch = np.zeros(10)\n",
    "\n",
    "                for k in range(20):\n",
    "                    running_count_final_epoch = evaluate(running_count_final_epoch, config, epoch, pipeline, mu_init, b_init, git_folder_path, accelerator, device, classifier_processor, classifier, classify = True, upload_images = False)\n",
    "\n",
    "                running_count_final_epoch = evaluate(running_count_final_epoch, config, epoch, pipeline, mu_init, b_init, git_folder_path, accelerator, device, classifier_processor, classifier, classify = True, upload_images = False, final_epoch = True)\n",
    "\n",
    "            if ((epoch + 1) % config.save_plot_epochs == 0 ) or (epoch == config.num_epochs - 1):\n",
    "                #draw samples and show them in a grid. Also update plots and add a class histogram and save model\n",
    "                running_count = evaluate(running_count, config, epoch, pipeline, mu_init, b_init, git_folder_path, accelerator, device, classifier_processor, classifier, classify = True)\n",
    "\n",
    "                #Save model\n",
    "                torch.save(model.state_dict(), 'models/trained_model_' + config.save_model_header + '.pt')\n",
    "\n",
    "                #######################plots of histories of important stuff#######################\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    legend = []\n",
    "\n",
    "                    for k in range(Nc):\n",
    "                        plt.plot(mu_hist[k, :])\n",
    "                        legend += [('mu_' + str(k + 1))]\n",
    "\n",
    "                    plt.legend(legend)\n",
    "                    plt.ylabel('mu')\n",
    "                    plt.xlabel('# dual update step')\n",
    "                    plt.title('dual variables history')\n",
    "                    plt.savefig('mu_hist.png')\n",
    "                    plt.close()\n",
    "\n",
    "                    torch.save(mu_hist, 'mu_hist.pt')\n",
    "\n",
    "                    up_to_hub('mu_hist.png', content_path = git_folder_path + '/mu_hist.png')\n",
    "                    up_to_hub('mu_hist.pt', content_path = git_folder_path + '/mu_hist.pt')\n",
    "\n",
    "                    #####################################################################################\n",
    "\n",
    "                    legend = []\n",
    "\n",
    "                    for k in range(Nc):\n",
    "                        plt.plot(b_hist[k, :])\n",
    "                        legend += [('b_' + str(k + 1))]\n",
    "    \n",
    "                    plt.ylabel('b')\n",
    "                    plt.xlabel('# dual update step')\n",
    "                    plt.title('constraint relaxations history')\n",
    "                    plt.savefig('b_hist.png')\n",
    "                    plt.close()\n",
    "\n",
    "                    torch.save(b_hist, 'b_hist.pt')\n",
    "\n",
    "                    up_to_hub('b_hist.png', content_path = git_folder_path + '/b_hist.png')\n",
    "                    up_to_hub('b_hist.pt', content_path = git_folder_path + '/b_hist.pt')\n",
    "\n",
    "                    #####################################################################################\n",
    "\n",
    "                    legend = []\n",
    "\n",
    "                    for k in range(Nc):\n",
    "                        plt.plot(constraint_hist[k, :])\n",
    "                        legend += [('(l-b)_' + str(k + 1))]\n",
    "\n",
    "                    plt.legend(legend)\n",
    "                    plt.ylabel('l-b')\n",
    "                    plt.xlabel('# dual update step')\n",
    "                    plt.title('constraint violations history')\n",
    "                    plt.savefig('constraint_hist.png')\n",
    "                    plt.close()\n",
    "\n",
    "                    torch.save(constraint_hist, 'constraint_hist.pt')\n",
    "\n",
    "                    up_to_hub('constraint_hist.png', content_path = git_folder_path + '/constraint_hist.png')\n",
    "                    up_to_hub('constraint_hist.pt', content_path = git_folder_path + '/constraint_hist.pt')\n",
    "\n",
    "                    #####################################################################################\n",
    "                    plt.plot(lagrangian_hist)\n",
    "                    plt.ylabel('lagrangian')\n",
    "                    plt.xlabel('# dual update step')\n",
    "                    plt.title('lagrangian history')\n",
    "                    plt.savefig('lagrangian_hist.png')\n",
    "                    plt.close()\n",
    "\n",
    "                    torch.save(lagrangian_hist, 'lagrangian_hist.pt')\n",
    "\n",
    "                    up_to_hub('lagrangian_hist.png', content_path = git_folder_path + '/lagrangian_hist.png')\n",
    "                    up_to_hub('lagrangian_hist.pt', content_path = git_folder_path + '/lagrangian_hist.pt')\n",
    "\n",
    "                    #####################################################################################\n",
    "                    \n",
    "                    for c in range(10):\n",
    "                        \n",
    "                        if c != 4:\n",
    "                            plt.plot(running_count_hist[:, c], alpha = 0.25)\n",
    "                        elif c == 4:\n",
    "                            plt.plot(running_count_hist[:, c], alpha = 1)\n",
    "\n",
    "                    plt.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "                    plt.ylabel('number of Generations')\n",
    "                    plt.xlabel('# dual update step')\n",
    "                    plt.title('number of Generations')\n",
    "                    plt.savefig('running_count_hist.png')\n",
    "                    plt.close()\n",
    "\n",
    "                    torch.save(running_count_hist, 'running_count_hist.pt')\n",
    "\n",
    "                    up_to_hub('running_count_hist.png', content_path = git_folder_path + '/running_count_hist.png')\n",
    "                    up_to_hub('running_count_hist.pt', content_path = git_folder_path + '/running_count_hist.pt')\n",
    "\n",
    "\n",
    "\n",
    "            if ((epoch + 1) % (config.save_image_epochs*config.running_average_length) == 0):\n",
    "\n",
    "                running_count_hist[j, :] = running_count\n",
    "                j += 1\n",
    "                running_count[:] = 0\n",
    "\n",
    "\n",
    "        if ((epoch + 1)%config.primal_per_dual == 0):\n",
    "            i_hist += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "\n",
    "######################################################################\n",
    "# An example of dataset creation for an adaptation task\n",
    "# N_datasets = 1 # a single dataset for the model to adapt to\n",
    "# N_spc = np.zeros((N_datasets, 10)) # a tensor specifying the number of Samples Per Class (spc) for each dataset\n",
    "# N_spc[0, 3] = 256\n",
    "######################################################################\n",
    "\n",
    "######################################################################\n",
    "# An example of dataset creation for minority class constraints setting\n",
    "N_datasets = 4 # 1 dataset for the main objective + 1 for each constraint\n",
    "N_spc = np.zeros((N_datasets, 10)) # a tensor specifying the number of Samples Per Class (spc) for each dataset\n",
    "N_spc[0, :] = 256\n",
    "\n",
    "#underrepresented classes have fewer training samples\n",
    "N_spc[0, 4] = 64\n",
    "N_spc[0, 5] = 64\n",
    "N_spc[0, 7] = 64\n",
    "\n",
    "#each constraint dataset consists of samples from a single minority class\n",
    "N_spc[1, 4] = 64\n",
    "N_spc[2, 5] = 64\n",
    "N_spc[3, 7] = 64\n",
    "######################################################################\n",
    "\n",
    "Train_dataloaders = create_datasets_fast(N_spc, config, False, False)\n",
    "################HYPERPARAMS##################\n",
    "eta_resilience = 0.1\n",
    "\n",
    "config.num_epochs = 500\n",
    "config.batches_per_epoch = int((np.sum(N_spc[0, :])/config.primal_batch_size)/2)\n",
    "if config.batches_per_epoch == 0:\n",
    "    config.batches_per_epoch = 1\n",
    "# config.batches_per_epoch = 1\n",
    "config.primal_per_dual = 2\n",
    "config.save_image_epochs = 100\n",
    "config.save_plot_epochs = 500\n",
    "config.running_average_length = 5\n",
    "config.lr_primal = 0.0001\n",
    "\n",
    "# mu_init_scalar = 0\n",
    "mu_init_scalar = 0.0\n",
    "b_init_scalar = 0.0\n",
    "\n",
    "if config.adaptation == True:\n",
    "    b_init = b_init_scalar*torch.ones(2)\n",
    "    mu_init = torch.tensor([1, 0])\n",
    "else:\n",
    "    mu_init = mu_init_scalar*torch.zeros(N_datasets)\n",
    "    mu_init[0] = 1\n",
    "    b_init = b_init_scalar*torch.ones(N_datasets)\n",
    "\n",
    "constrained = True\n",
    "resilient = True\n",
    "\n",
    "alpha = 0.085\n",
    "\n",
    "#OPTIMIZER + NOISE SCHEDULER + LR_SCHEDULER\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps = 1000)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = config.lr_primal)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = config.lr_warmup_steps,\n",
    "    num_training_steps = (config.batches_per_epoch * config.num_epochs),\n",
    "    num_cycles = 0.25\n",
    ")\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "git_folder_path = 'FOLDER_NAME'\n",
    "config.save_model_header = 'MODEL_NAME'\n",
    "\n",
    "if config.wandb_logging == True:\n",
    "    wandb.init(\n",
    "        project = 'PROJECT_NAME',\n",
    "        config = config\n",
    "    )\n",
    "\n",
    "# Open the file in write mode and save the string\n",
    "with open('run_info.txt', 'w') as file:\n",
    "    file.write('num_epochs = ' + str(config.num_epochs) + \"\\n\")\n",
    "    file.write('batches_per_epoch = ' + str(config.batches_per_epoch) + \"\\n\")\n",
    "    file.write('primal_batch_size = ' + str(config.primal_batch_size) + \"\\n\")\n",
    "    file.write('dataset_size = ' + str(np.sum(N_spc[0, :]))  + \"\\n\")\n",
    "    file.write('primal_per_dual = ' + str(config.primal_per_dual) + \"\\n\")\n",
    "    file.write('save_image_epochs = ' + str(config.save_image_epochs) + \"\\n\")\n",
    "    file.write('save_plot_epochs = ' + str(config.save_plot_epochs) + \"\\n\")\n",
    "    file.write('running_average_length_for_classification_of_generated_samples = ' + str(config.running_average_length) + \"\\n\")\n",
    "    file.write('lr_dual_to_primal = ' + str(config.lr_dual_to_primal) + \"\\n\")\n",
    "    file.write('eta_resilience = ' + str(eta_resilience) + \"\\n\")\n",
    "    file.write('eta_main = ' + str(config.lr_primal)  + \"\\n\")\n",
    "    file.write('alpha (const relaxation cost) = ' + str(alpha) + \"\\n\")\n",
    "    file.write('initial mu = ' + str(mu_init) + \"\\n\")\n",
    "    file.write('initial b = ' + str(b_init) + \"\\n\")\n",
    "    file.write('save_model_header = ' + str(config.save_model_header)  + \"\\n\")\n",
    "    file.write('load_model_header = ' + str(config.load_model_header)  + \"\\n\")\n",
    "    file.write('constrained = ' + str(constrained)  + \"\\n\")\n",
    "    file.write('resilient = ' + str(resilient)  + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "up_to_hub('run_info.txt', content_path = git_folder_path + '/run_info.txt')\n",
    "\n",
    "# def train_loop(config, git_folder_path, model, classifier, noise_scheduler, optimizer, train_dataloaders, lr_scheduler, mu_init, b_init, constrained = False, resilient = False, alpha = 0,  model_pretrained = None):\n",
    "if config.adaptation == True:\n",
    "    args = (config, git_folder_path, model, classifier, noise_scheduler, optimizer, Train_dataloaders, lr_scheduler, mu_init, b_init, constrained, resilient, alpha, model_pretrained)\n",
    "else:\n",
    "    args = (config, git_folder_path, model, classifier, noise_scheduler, optimizer, Train_dataloaders, lr_scheduler, mu_init, b_init, constrained, resilient, alpha)\n",
    "\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes = config.num_gpus)\n",
    "\n",
    "#Ignore if not having issues with server ports\n",
    "# portnum = 8000\n",
    "# notebook_launcher(train_loop, args, num_processes = config.num_gpus, use_port= str(portnum))\n",
    "\n",
    "if config.wandb_logging == True:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
